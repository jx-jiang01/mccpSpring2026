\begin{thebibliography}{10}
\expandafter\ifx\csname url\endcsname\relax
  \def\url#1{\texttt{#1}}\fi
\expandafter\ifx\csname urlprefix\endcsname\relax\def\urlprefix{URL }\fi
\expandafter\ifx\csname href\endcsname\relax
  \def\href#1#2{#2} \def\path#1{#1}\fi

\bibitem{caponnetto2007optimal}
A.~Caponnetto, E.~De~Vito, Optimal rates for the regularized least-squares algorithm, Foundations of Computational Mathematics 7~(3) (2007) 331--368.

\bibitem{rudi2015less}
A.~Rudi, R.~Camoriano, L.~Rosasco, Less is more: Nystr{\"o}m computational regularization., in: NIPS, 2015, pp. 1657--1665.

\bibitem{zhang2015divide}
Y.~Zhang, J.~Duchi, M.~Wainwright, Divide and conquer kernel ridge regression: A distributed algorithm with minimax optimal rates, Journal of Machine Learning Research 16~(1) (2015) 3299--3340.

\bibitem{meister2016optimal}
M.~Meister, I.~Steinwart, Optimal learning rates for localized svms, Journal of Machine Learning Research 17~(1) (2016) 6722--6765.

\bibitem{lin2019boosted}
S.-B. Lin, Y.~Lei, D.-X. Zhou, Boosted kernel ridge regression: optimal learning rates and early stopping, Journal of Machine Learning Research 20~(1) (2019) 1738--1773.

\bibitem{gerfo2008spectral}
L.~L. Gerfo, L.~Rosasco, F.~Odone, E.~D. Vito, A.~Verri, Spectral algorithms for supervised learning, Neural Computation 20~(7) (2008) 1873--1897.

\bibitem{cucker2002best}
F.~Cucker, S.~Smale, Best choices for regularization parameters in learning theory: On the bias-variance problem, Foundations of Computational Mathematics 2 (2002) 413--428.

\bibitem{christmann2018total}
A.~Christmann, D.~Xiang, D.-X. Zhou, Total stability of kernel methods, Neurocomputing 289 (2018) 101--118.

\bibitem{kohler2022total}
H.~K{\"o}hler, A.~Christmann, Total stability of svms and localized svms, Journal of Machine Learning Research 23~(1) (2022) 4305--4345.

\bibitem{celisse2021analyzing}
A.~Celisse, M.~Wahl, Analyzing the discrepancy principle for kernelized spectral filter learning algorithms., Journal of Machine Learning Research 22 (2021) 76--1.

\bibitem{lu2020balancing}
S.~Lu, P.~Math{\'e}, S.~V. Pereverzev, Balancing principle in supervised learning for a general regularization scheme, Applied and Computational Harmonic Analysis 48~(1) (2020) 123--148.

\bibitem{blanchard2019lepskii}
G.~Blanchard, P.~Math{\'e}, N.~M{\"u}cke, Lepskii principle in supervised learning, arXiv preprint arXiv:1905.10764 (2019).

\bibitem{gyorfi2002distribution}
L.~Gy{\"o}rfi, M.~Kohler, A.~Krzy{\.z}ak, H.~Walk, A Distribution-free Theory of Nonparametric Regression, Vol.~1, Springer, 2002.

\bibitem{caponnetto2010cross}
A.~Caponnetto, Y.~Yao, Cross-validation based adaptation for regularization operators in learning theory, Analysis and Applications 8~(02) (2010) 161--183.

\bibitem{engl1996regularization}
H.~W. Engl, M.~Hanke, A.~Neubauer, Regularization of Inverse Problems, Vol. 375, Springer Science \& Business Media, 1996.

\bibitem{lepskii1991problem}
O.~Lepskii, On a problem of adaptive estimation in gaussian white noise, Theory of Probability \& Its Applications 35~(3) (1991) 454--466.

\bibitem{de2010adaptive}
E.~De~Vito, S.~Pereverzyev, L.~Rosasco, Adaptive kernel methods using the balancing principle, Foundations of Computational Mathematics 10~(4) (2010) 455--479.

\bibitem{smale2005shannon}
S.~Smale, D.-X. Zhou, Shannon sampling {II}: Connections to learning theory, Applied and Computational Harmonic Analysis 19~(3) (2005) 285--302.

\bibitem{smale2007learning}
S.~Smale, D.-X. Zhou, Learning theory estimates via integral operators and their approximations, Constructive Approximation 26~(2) (2007) 153--172.

\bibitem{steinwart2009optimal}
I.~Steinwart, D.~R. Hush, C.~Scovel, et~al., Optimal rates for regularized least squares regression., in: COLT, 2009, pp. 79--93.

\bibitem{lin2017distributed}
S.-B. Lin, X.~Guo, D.-X. Zhou, Distributed learning with regularized least squares, Journal of Machine Learning Research 18~(92) (2017) 3202--3232.

\bibitem{cucker2007learning}
F.~Cucker, D.~X. Zhou, Learning Theory: An Approximation Theory Viewpoint, Vol.~24, Cambridge University Press, 2007.

\bibitem{steinwart2008support}
I.~Steinwart, A.~Christmann, Support Vector Machines, Springer Science \& Business Media, 2008.

\bibitem{blanchard2018optimal}
G.~Blanchard, N.~M{\"u}cke, Optimal rates for regularization of statistical inverse learning problems, Foundations of Computational Mathematics 18~(4) (2018) 971--1013.

\bibitem{chang2017distributed}
X.~Chang, S.-B. Lin, D.-X. Zhou, Distributed semi-supervised learning with kernel ridge regression, Journal of Machine Learning Research 18~(1) (2017) 1493--1514.

\bibitem{blanchard2016convergence}
G.~Blanchard, N.~Kr{\"a}mer, Convergence rates of kernel conjugate gradient for random design regression, Analysis and Applications 14~(06) (2016) 763--794.

\bibitem{lu2019analysis}
S.~Lu, P.~Math{\'e}, S.~Pereverzyev~Jr, Analysis of regularized {N}ystr{\"o}m subsampling for regression functions of low smoothness, Analysis and Applications 17~(06) (2019) 931--946.

\bibitem{fischer2020sobolev}
S.~Fischer, I.~Steinwart, Sobolev norm learning rates for regularized least-squares algorithms., Journal of Machine Learning Research 21~(205) (2020) 1--38.

\bibitem{guo2017learning}
Z.-C. Guo, S.-B. Lin, D.-X. Zhou, Learning theory of distributed spectral algorithms, Inverse Problems 33~(7) (2017) 074009.

\bibitem{raskutti2014early}
G.~Raskutti, M.~J. Wainwright, B.~Yu, Early stopping and non-parametric regression: an optimal data-dependent stopping rule, Journal of Machine Learning Research 15~(1) (2014) 335--366.

\bibitem{smale2004shannon}
S.~Smale, D.-X. Zhou, Shannon sampling and function reconstruction from point values, Bulletin of the American Mathematical Society 41~(3) (2004) 279--305.

\bibitem{lin2020distributed}
S.-B. Lin, D.~Wang, D.-X. Zhou, Distributed kernel ridge regression with communications., Journal of Machine Learning Research 21 (2020) 93--1.

\bibitem{bhatia2013matrix}
R.~Bhatia, Matrix Analysis, Vol. 169, Springer Science \& Business Media, 2013.

\end{thebibliography}
